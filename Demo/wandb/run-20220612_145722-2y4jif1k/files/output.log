network: MLP(
  (layer1): Linear(in_features=470, out_features=256, bias=True)
  (layer2): Linear(in_features=256, out_features=64, bias=True)
  (out): Linear(in_features=64, out_features=1, bias=True)
)




















  3%|▎         | 51/2000 [00:41<25:14,  1.29it/s]


  3%|▎         | 59/2000 [00:47<26:13,  1.23it/s]
Traceback (most recent call last):
  File "D:\Projects\DeepLearning\Demo\MLP简单版.py", line 143, in <module>
    train_ls, valid_ls = train(net, train_features, train_labels, None, None, num_epochs, lr, weight_decay, batch_size)
  File "D:\Projects\DeepLearning\Demo\MLP简单版.py", line 120, in train
    optimizer.step()
  File "C:\Users\19423\.conda\envs\nlp\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\19423\.conda\envs\nlp\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\19423\.conda\envs\nlp\lib\site-packages\torch\optim\adam.py", line 141, in step
    F.adam(params_with_grad,
  File "C:\Users\19423\.conda\envs\nlp\lib\site-packages\torch\optim\_functional.py", line 110, in adam
    param.addcdiv_(exp_avg, denom, value=-step_size)
KeyboardInterrupt