{
 "cells": [
  {
   "cell_type": "raw",
   "id": "literary-debate",
   "metadata": {},
   "source": [
    "[[ch11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-rally",
   "metadata": {},
   "source": [
    "# Productionization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-evolution",
   "metadata": {},
   "source": [
    "## Data Scientists, Engineers, and Analysts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-young",
   "metadata": {},
   "source": [
    "### Prototyping, Deployment, and Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-father",
   "metadata": {},
   "source": [
    "### Notebooks and Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-origin",
   "metadata": {},
   "source": [
    "## Databricks: Your Unified Data Analytics Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-inflation",
   "metadata": {},
   "source": [
    "### Support for Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-mineral",
   "metadata": {},
   "source": [
    "### Support For Multiple Programming Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-association",
   "metadata": {},
   "source": [
    "### Support For ML Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-paint",
   "metadata": {},
   "source": [
    "### Support for Model Repository, Access Control, Data Lineage, and Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-premium",
   "metadata": {},
   "source": [
    "## Databricks Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-democrat",
   "metadata": {},
   "source": [
    "### Set Up Access to S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-howard",
   "metadata": {},
   "source": [
    "### Set Up Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-oasis",
   "metadata": {},
   "source": [
    "### Create Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-ordering",
   "metadata": {},
   "source": [
    "### Create Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-racing",
   "metadata": {},
   "source": [
    "```python\n",
    "# Make directory in DBFS\n",
    "dbutils.fs.mkdirs(\"dbfs:/databricks/models/spacy\")\n",
    "\n",
    "# Copy files from S3 to DBFS\n",
    "dbutils.fs.cp(\"s3a://nlp-demo/models/spacy/\",\n",
    "              \"dbfs:/databricks/models/spacy/\", True)\n",
    "\n",
    "# Confirm files in DBFS\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/models/spacy/\"))\n",
    "\n",
    "# Make directory in DBFS\n",
    "dbutils.fs.mkdirs(\"dbfs:/databricks/scripts/\")\n",
    "\n",
    "# Put script in DBFS\n",
    "dbutils.fs.put(\"dbfs:/databricks/scripts/spacy_with_models.sh\", \\\n",
    "\"\"\"pip install /dbfs/databricks/models/spacy/en_core_web_lg-2.3.1.tar.gz \\\n",
    "pip install /dbfs/databricks/models/spacy/en_ner_base_V3-0.0.0.tar.gz \\\n",
    "pip install /dbfs/databricks/models/spacy/\\\n",
    "en_textcat_prodigy_V3_base_full-0.0.0.tar.gz\"\"\", True)\n",
    "\n",
    "# Confirm file in DBFS\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/scripts/spacy_with_models.sh\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-encoding",
   "metadata": {},
   "source": [
    "### Enable Init Script and Restart Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-christian",
   "metadata": {},
   "source": [
    "### Run Speed Test - Inference on NER using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "# Python\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "inputPath = \"s3a://nlp-demo/ag_dataset/prepared/train_prepared.csv\" \\\n",
    " # path to your S3 bucket\n",
    "df = spark.read.format('csv').options(header='true', inferSchema='true', \\\n",
    " quote=\"\\\"\", escape= \"\\\"\").load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View shape of data\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"text\", StringType(), False),\n",
    "    StructField(\"start_char\", IntegerType(), False),\n",
    "    StructField(\"end_char\", IntegerType(), False),\n",
    "    StructField(\"label\", StringType(), False)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Get Entities\n",
    "def get_entities(text):\n",
    "    global nlp\n",
    "    try:\n",
    "        doc = nlp(str(text))\n",
    "    except:\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        doc = nlp(str(text))\n",
    "    return [[e.text, e.start_char, e.end_char, e.label_] for e in doc.ents]\n",
    "\n",
    "get_entities_udf = udf(get_entities, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Entities\n",
    "documents_df = df.withColumn('entities', get_entities_udf('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write parquet\n",
    "documents_df.write.parquet(\\\n",
    " \"s3a://nlp-demo/ag_dataset/prepared/write_test.parquet\", \\\n",
    " mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "'''Main Libraries'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "write_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "\n",
    "# Install SpaCy\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    " \n",
    "# Start timer\n",
    "start_time = time.time()\n",
    " \n",
    "# Define function to read data\n",
    "def read_data(file):\n",
    "    read_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "    data = pd.read_csv(read_path+file)\n",
    "    return data\n",
    " \n",
    "# Read data\n",
    "data = read_data('/data/ag_dataset/prepared/train_prepared.csv')\n",
    " \n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    " \n",
    "# Load time\n",
    "load_time = time.time()\n",
    "print(\"Time to load data and model: \", np.round(load_time-start_time,2))\n",
    " \n",
    "# Apply NLP model\n",
    "data[\"entities\"] = data[\"description\"].apply(lambda x: \\\n",
    " [(e.text, e.start_char, e.end_char, e.label_) for e in nlp(x).ents])\n",
    " \n",
    "# End timer\n",
    "end_time = time.time()\n",
    "print(\"Time to perform NER: \", np.round(end_time-load_time,2))\n",
    "print(\"Total time: \", np.round(time.time()-start_time,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-blogger",
   "metadata": {},
   "source": [
    "## Machine Learning Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-object",
   "metadata": {},
   "source": [
    "### Production Pipeline Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "# Python\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "inputPath = getArgument(\"inputPath\", \"default\")\n",
    "df = spark.read.format('csv').options(header='true', inferSchema='true', \\\n",
    " quote=\"\\\"\", escape= \"\\\"\").load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Schema\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"text\", StringType(), False),\n",
    "    StructField(\"start_char\", IntegerType(), False),\n",
    "    StructField(\"end_char\", IntegerType(), False),\n",
    "    StructField(\"label\", StringType(), False)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Get Entities\n",
    "def get_entities(text):\n",
    "    global nlp\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except:\n",
    "        nlp = spacy.load('en_ner_base_V3')\n",
    "        doc = nlp(text)\n",
    "    return [[e.text, e.start_char, e.end_char, e.label_] for e in doc.ents]\n",
    "\n",
    "get_entities_udf = udf(lambda x: get_entities(x), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Entities\n",
    "documents_df = df.withColumn('entities', get_entities_udf('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Parquet\n",
    "outPath = getArgument(\"outputPath\", \"default\")\n",
    "documents_df.write.format(\"parquet\").mode(\"overwrite\").save(outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-linux",
   "metadata": {},
   "source": [
    "### Scheduled Machine Learning Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-railway",
   "metadata": {},
   "source": [
    "### Event-Driven Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "const https = require(\"https\");\n",
    "\n",
    "exports.handler = (event, context, callback) => {\n",
    "  var data = JSON.stringify({\n",
    "    \"job_id\": XXX\n",
    "  });\n",
    "\n",
    "  var options = {\n",
    "     host: \"XXX-XXXXXXX-XXX.cloud.databricks.com\",\n",
    "     port: 443,\n",
    "     path: \"/api/2.0/jobs/run-now\",\n",
    "     method: \"POST\",\n",
    "     // authentication headers\n",
    "     headers: {\n",
    "      \"Authorization\": \"Bearer XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Content-Length\": Buffer.byteLength(data)\n",
    "     }\n",
    "  };\n",
    "\n",
    "  var request = https.request(options, function(res){\n",
    "    var body = \"\";\n",
    "\n",
    "    res.on(\"data\", function(data) {\n",
    "      body += data;\n",
    "    });\n",
    "\n",
    "    res.on(\"end\", function() {\n",
    "      console.log(body);\n",
    "    });\n",
    "\n",
    "    res.on(\"error\", function(e) {\n",
    "      console.log(\"Got error: \" + e.message);\n",
    "    });\n",
    "\n",
    "  });\n",
    "\n",
    "  request.write(data);\n",
    "  request.end();\n",
    "};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-pantyhose",
   "metadata": {},
   "source": [
    "## MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-boring",
   "metadata": {},
   "source": [
    "### Log and Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "# SpaCY\n",
    "import spacy \n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.spacy\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_textcat_prodigy_V3_base_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metadata\n",
    "nlp.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow Tracking\n",
    "with mlflow.start_run(run_name='SpaCy-TextCat-Prodigy-V3-Base-Full'):\n",
    "    mlflow.set_tag('model_flavor', 'spacy')\n",
    "    mlflow.spacy.log_model(spacy_model=nlp, artifact_path='model')\n",
    "    mlflow.log_metric('textcat_score', 91.774875419)\n",
    "    my_run_id = mlflow.active_run().info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-costume",
   "metadata": {},
   "source": [
    "### MLflow Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "# Define function to read data\n",
    "def read_data(file):\n",
    "    read_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "    data = pd.read_csv(read_path+file)\n",
    "    return data\n",
    " \n",
    "# Read data\n",
    "data = read_data('/data/ag_dataset/prepared/train_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON\n",
    "data.loc[:10,\"description\"].to_json(path_or_buf= \\\n",
    "        '/content/drive/My Drive/Applied-NLP-in-the-Enterprise/data/\\\n",
    "        ag_dataset/prepared/sample.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Model - CURL\n",
    "MODEL_VERSION_URI = XXXXXX #the model path\n",
    "DATABRICKS_TOKEN = XXXXXX #secret access token\n",
    "JSON_PATH = XXXXXX #path to the JSON we created earlier in Colab\n",
    " \n",
    "!curl -u token:$DATABRICKS_TOKEN -H \\\n",
    " \"Content-Type: application/json; format=pandas-records\" \\\n",
    " -d@$JSON_PATH $MODEL_VERSION_URI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Call the Model in Python\n",
    "import requests\n",
    " \n",
    "def score_model(model_uri, databricks_token, data):\n",
    "    headers = {\n",
    "        \"Authorization\": 'Bearer '+ databricks_token,\n",
    "        \"Content-Type\": \"application/json; format=pandas-records\",\n",
    "      }\n",
    "    data_json = data if isinstance(data, list) else data.to_list()\n",
    "    response = requests.request(method='POST', headers=headers,\n",
    "        url=model_uri, json=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code},\n",
    "            {response.text}\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the Model\n",
    "MODEL_VERSION_URI = XXXXXX # the model path\n",
    "DATABRICKS_TOKEN = XXXXXX # secret access token\n",
    " \n",
    "score_model(MODEL_VERSION_URI, DATABRICKS_TOKEN, data.loc[:10,\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-throat",
   "metadata": {},
   "source": [
    "## Alternatives to Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-device",
   "metadata": {},
   "source": [
    "### Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-client",
   "metadata": {},
   "source": [
    "### Saturn Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-lawyer",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
