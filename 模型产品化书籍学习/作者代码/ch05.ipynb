{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[ch05]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings: How Machines \"Understand\" Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand vs. Reading Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "o = torch.zeros(20000).int()\n",
    "o[1152] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = torch.nn.Embedding(num_embeddings = 20000, embedding_dim = 300)\n",
    "e = E(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings in the Age of Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from torchtext import *\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors='glove.6B.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up fields\n",
    "TEXT = data.Field(lower=True, include_lengths=True, \\\n",
    "batch_first=False, tokenize='spacy')\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# make splits for data\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train, vectors='glove.6B.100d') \\\n",
    "# use 'glove.42B.300d' for greater accuracy or \\\n",
    "'glove.6B.100d' for greater speed\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), \\\n",
    "batch_sizes=(128,1024), device=dev, sort_within_batch=True, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classifier(nn.Module):\n",
    "    def __init__(self, embedding_size = 100, hidden_size = 512, num_layers = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up an embedding layer with the right dimensions, \\\n",
    "        and copy the weights from the pretrained glove embeddings\n",
    "        vocab = TEXT.vocab\n",
    "        self.embed = nn.Embedding(len(vocab), embedding_size).cuda()\n",
    "        self.embed.weight.data.copy_(vocab.vectors)\n",
    "\n",
    "        # Set up a standard PyTorch RNN sections with the right \\\n",
    "        dimensions and a variable number of layers\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers)\n",
    "\n",
    "        # Add a two layer classification head with the right dimensions. \\\n",
    "        The final layer must output a single number\n",
    "        self.classificationLayer1 = nn.Linear(hidden_size,10)\n",
    "        self.classificationLayer2 = nn.Linear(10,1)\n",
    "\n",
    "\n",
    "    def forward(self, input, lengths=None):\n",
    "\n",
    "        embed_input = self.embed(input)\n",
    "        packed_emb = nn.utils.rnn.pack_padded_sequence(embed_input, \\\n",
    "        lengths, batch_first=False)\n",
    "\n",
    "        output, hidden = self.rnn(packed_emb)\n",
    "        hidden = hidden[-1]\n",
    "        x = hidden.squeeze(0)\n",
    "        x = self.classificationLayer1(x)\n",
    "        x = self.classificationLayer2(x)\n",
    "\n",
    "        logits = x.view(-1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_classifier(hidden_size=256, num_layers=1)\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_iter:\n",
    "    (x,x_len) = batch.text\n",
    "    pred = model(x,x_len)\n",
    "    print(pred.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.binary_cross_entropy_with_logits\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, test_data):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(test_data):\n",
    "            text, text_lengths = batch_data.text\n",
    "            logits = model(text, text_lengths)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            total += batch_data.label.size(0)\n",
    "            correct += (predicted_labels == batch_data.label.long()).sum()\n",
    "        return correct.float()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_iter):\n",
    "        (x,x_lengths)=batch.text\n",
    "        pred = model(x,x_lengths)\n",
    "\n",
    "        actual=batch.label.float()\n",
    "        loss = loss_func(pred,actual)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    if (epoch==5):\n",
    "        for g in opt.param_groups:\n",
    "            g['lr'] = 3e-3\n",
    "\n",
    "    print(\"Accuracy: \" + str(get_metrics(model, test_iter).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    # based on:\n",
    "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
    "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "\n",
    "    tensor = torch.LongTensor(indexed).to(dev)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"I like that Far From Home is trying something new and that its\n",
    "humor  feels more real than the ironic cracks in most superhero movies.\n",
    "I just wish its good pieces all came together more satisfyingly.\"\"\"\n",
    "\n",
    "print('Probability positive:')\n",
    "predict_sentiment(model, review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Things That Aren't Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Sonnet in The MIDI Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some General Tips for Making Custom Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
