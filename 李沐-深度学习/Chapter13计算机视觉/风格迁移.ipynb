{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "d2l.set_figsize()\n",
    "content_img = d2l.Image.open('../img/rainier.jpg')\n",
    "d2l.plt.imshow(content_img);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "style_img = d2l.Image.open('../img/autumn-oak.jpg')\n",
    "d2l.plt.imshow(style_img);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img, image_shape):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_shape),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
    "    return transforms(img).unsqueeze(0)\n",
    "\n",
    "def postprocess(img):\n",
    "    img = img[0].to(rgb_std.device)\n",
    "    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)\n",
    "    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pretrained_net = torchvision.models.vgg19(pretrained=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "style_layers, content_layers = [0, 5, 10, 19, 28], [25]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = nn.Sequential(*[pretrained_net.features[i] for i in\n",
    "                      range(max(content_layers + style_layers) + 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_features(X, content_layers, style_layers):\n",
    "    contents = []\n",
    "    styles = []\n",
    "    for i in range(len(net)):\n",
    "        X = net[i](X)\n",
    "        if i in style_layers:\n",
    "            styles.append(X)\n",
    "        if i in content_layers:\n",
    "            contents.append(X)\n",
    "    return contents, styles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_contents(image_shape, device):\n",
    "    content_X = preprocess(content_img, image_shape).to(device)\n",
    "    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n",
    "    return content_X, contents_Y\n",
    "\n",
    "def get_styles(image_shape, device):\n",
    "    style_X = preprocess(style_img, image_shape).to(device)\n",
    "    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n",
    "    return style_X, styles_Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def content_loss(Y_hat, Y):\n",
    "    # 我们从动态计算梯度的树中分离目标：\n",
    "    # 这是一个规定的值，而不是一个变量。\n",
    "    return torch.square(Y_hat - Y.detach()).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gram(X):\n",
    "    num_channels, n = X.shape[1], X.numel() // X.shape[1]\n",
    "    X = X.reshape((num_channels, n))\n",
    "    return torch.matmul(X, X.T) / (num_channels * n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def style_loss(Y_hat, gram_Y):\n",
    "    return torch.square(gram(Y_hat) - gram_Y.detach()).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tv_loss(Y_hat):\n",
    "    return 0.5 * (torch.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +\n",
    "                  torch.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "content_weight, style_weight, tv_weight = 1, 1e3, 10\n",
    "\n",
    "def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n",
    "    # 分别计算内容损失、风格损失和全变分损失\n",
    "    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(\n",
    "        contents_Y_hat, contents_Y)]\n",
    "    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(\n",
    "        styles_Y_hat, styles_Y_gram)]\n",
    "    tv_l = tv_loss(X) * tv_weight\n",
    "    # 对所有损失求和\n",
    "    l = sum(10 * styles_l + contents_l + [tv_l])\n",
    "    return contents_l, styles_l, tv_l, l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SynthesizedImage(nn.Module):\n",
    "    def __init__(self, img_shape, **kwargs):\n",
    "        super(SynthesizedImage, self).__init__(**kwargs)\n",
    "        self.weight = nn.Parameter(torch.rand(*img_shape))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_inits(X, device, lr, styles_Y):\n",
    "    gen_img = SynthesizedImage(X.shape).to(device)\n",
    "    gen_img.weight.data.copy_(X.data)\n",
    "    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)\n",
    "    styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
    "    return gen_img(), styles_Y_gram, trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\n",
    "    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs],\n",
    "                            legend=['content', 'style', 'TV'],\n",
    "                            ncols=2, figsize=(7, 2.5))\n",
    "    for epoch in range(num_epochs):\n",
    "        trainer.zero_grad()\n",
    "        contents_Y_hat, styles_Y_hat = extract_features(\n",
    "            X, content_layers, style_layers)\n",
    "        contents_l, styles_l, tv_l, l = compute_loss(\n",
    "            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.axes[1].imshow(postprocess(X))\n",
    "            animator.add(epoch + 1, [float(sum(contents_l)),\n",
    "                                     float(sum(styles_l)), float(tv_l)])\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device, image_shape = d2l.try_gpu(), (300, 450)\n",
    "net = net.to(device)\n",
    "content_X, contents_Y = get_contents(image_shape, device)\n",
    "_, styles_Y = get_styles(image_shape, device)\n",
    "output = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}